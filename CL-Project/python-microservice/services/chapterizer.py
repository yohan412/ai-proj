# services/chapterizer.py
from typing import List, Dict, Any, Optional, Tuple
import os
import threading
import torch
import sys
import io
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig        # â˜… NEW: BnB 4bit ì„¤ì • ì‚¬ìš©
from transformers import AutoConfig                # â˜… NEW: ë ˆí¬ config ë¡œë“œ í›„ mxfp4 ì„¤ì • ì œê±°

from utils.helpers import round_time, ensure_json

# Windows í•œê¸€ ì¸ì½”ë”©ì€ app.pyì—ì„œ ì²˜ë¦¬ë¨

# ì „ì—­ íŒŒì´í”„ ìºì‹œ
_pipe = None
_loaded_key: Tuple[str, bool, bool, str, str, str, bool, str] = ("", False, False, "", "", "", False, "")
_lock = threading.Lock()

def _lang_label_from_code(code: Optional[str]) -> str:
    code = (code or "").lower()
    table = {
        "ko": "Korean", "en": "English", "ja": "Japanese",
        "zh": "Chinese", "zh-cn": "Chinese", "zh-tw": "Chinese (Traditional)",
        "es": "Spanish", "fr": "French", "de": "German", "it": "Italian",
        "pt": "Portuguese", "ru": "Russian", "vi": "Vietnamese",
        "id": "Indonesian", "th": "Thai", "hi": "Hindi", "ar": "Arabic",
    }
    return table.get(code, "the same language as the transcript")

def _str2dtype(name: str):
    name = (name or "auto").lower()
    if name == "float16": return torch.float16
    if name == "bfloat16": return torch.bfloat16
    if name == "float32": return torch.float32
    # â˜… CHANGED: auto â†’ CUDAë©´ bfloat16, ì•„ë‹ˆë©´ float32
    if torch.cuda.is_available():
        print(f"[chapterizer] GPU detected: {torch.cuda.get_device_name(0)}")
        print(f"[chapterizer] GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        return torch.bfloat16
    else:
        print("[chapterizer] No GPU detected, using CPU with float32")
        return torch.float32

def _make_bnb_config() -> BitsAndBytesConfig:
    """BitsAndBytes 4bit(NF4) ì„¤ì • (CUDA ì „ìš©)."""
    return BitsAndBytesConfig(               # â˜… NEW
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

def _get_pipe(
    model_id: str,
    load_in_4bit: bool,           # â˜… CHANGED: ì‹œê·¸ë‹ˆì²˜ëŠ” ìœ ì§€(í™˜ê²½ë³€ìˆ˜ ì—°ë™ìš©), ì‹¤ì œ ì „ë‹¬ì€ quantization_configë¡œë§Œ
    temperature: float,
    max_new_tokens: int,
    hf_token: Optional[str],
    max_gpu_mem: str,
    max_cpu_mem: str,
    offload_dir: str,
    low_cpu_mem: bool,
    torch_dtype_name: str
):
    """í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì¤€ë¹„(ìºì‹œë¨)."""
    global _pipe, _loaded_key

    want = (
        model_id, load_in_4bit, torch.cuda.is_available(),
        max_gpu_mem, max_cpu_mem, offload_dir, low_cpu_mem, torch_dtype_name
    )
    if _pipe is not None and _loaded_key == want:
        return _pipe

    try:
        with _lock:
            if _pipe is not None and _loaded_key == want:
                return _pipe

        dtype = _str2dtype(torch_dtype_name)

        # ë””ë°”ì´ìŠ¤/ë©”ëª¨ë¦¬ ë§¤í•‘
        if torch.cuda.is_available():
            # GPU ì‚¬ìš© ì‹œ ìµœì í™”ëœ ì„¤ì •
            max_memory = {0: str(max_gpu_mem), "cpu": str(max_cpu_mem)}
            device_map = "auto"
            print(f"[chapterizer] Using GPU with {max_gpu_mem} GPU memory, {max_cpu_mem} CPU memory")
        else:
            # CPU ì‚¬ìš© ì‹œ ì„¤ì •
            max_memory = {"cpu": str(max_cpu_mem)}
            device_map = "cpu"
            print(f"[chapterizer] Using CPU with {max_cpu_mem} CPU memory")

        common_kw = dict(
            device_map=device_map,
            max_memory=max_memory,
            low_cpu_mem_usage=bool(low_cpu_mem),
            trust_remote_code=True,
            token=(hf_token or None),
            offload_folder=offload_dir,
        )

        # Llama-3.1-8B-Instruct ëª¨ë¸ìš© í† í¬ë‚˜ì´ì € ì„¤ì •
        tok = AutoTokenizer.from_pretrained(
            model_id, 
            use_fast=True, 
            trust_remote_code=True, 
            token=(hf_token or None)
        )
        
        # Llama ëª¨ë¸ìš© íŒ¨ë”© í† í° ì„¤ì •
        if tok.pad_token is None:
            tok.pad_token = tok.eos_token

        # ëª¨ë¸ ë¡œë“œ: 4bit(BnB) ìš°ì„  ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ FP í´ë°±
        print(f"[chapterizer] ëª¨ë¸ ë¡œë”© ì‹œì‘ - 4bit: {load_in_4bit}, CUDA: {torch.cuda.is_available()}")
        mdl = None
        if torch.cuda.is_available() and load_in_4bit:
            try:
                print(f"[chapterizer] 4bit ëª¨ë¸ ë¡œë”© ì‹œë„...")
                bnb_cfg = _make_bnb_config()
                print(f"[chapterizer] BnB ì„¤ì • ì™„ë£Œ, ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                print(f"[chapterizer] AutoModelForCausalLM.from_pretrained í˜¸ì¶œ ì‹œì‘...")
                print(f"[chapterizer] model_id: {model_id}")
                print(f"[chapterizer] torch_dtype: torch.bfloat16")
                print(f"[chapterizer] common_kw keys: {list(common_kw.keys())}")
                
                try:
                    mdl = AutoModelForCausalLM.from_pretrained(
                        model_id,
                        torch_dtype=torch.bfloat16,
                        quantization_config=bnb_cfg,
                        **common_kw
                    )
                    print(f"[chapterizer] AutoModelForCausalLM.from_pretrained í˜¸ì¶œ ì™„ë£Œ!")
                except Exception as load_error:
                    print(f"[chapterizer] from_pretrained ë‚´ë¶€ ì˜¤ë¥˜: {load_error}")
                    import traceback
                    traceback.print_exc()
                    raise load_error
                
                print(f"[chapterizer] 4bit ëª¨ë¸ ë¡œë”© ì„±ê³µ, íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
                print(f"[chapterizer] 4bit íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œì‘...")
                
                # íŒŒì´í”„ë¼ì¸ ìƒì„± ì „ í† í¬ë‚˜ì´ì € ê²€ì¦
                print(f"[chapterizer] í† í¬ë‚˜ì´ì € ê²€ì¦ - pad_token_id: {tok.pad_token_id}, eos_token_id: {tok.eos_token_id}")
                
                _pipe = pipeline(
                    "text-generation",
                    model=mdl, tokenizer=tok,
                    temperature=temperature, 
                    max_new_tokens=max_new_tokens,
                    do_sample=True, 
                    return_full_text=False,
                    pad_token_id=tok.eos_token_id
                )
                print(f"[chapterizer] 4bit íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
                print(f"[chapterizer] íŒŒì´í”„ë¼ì¸ ê°ì²´ íƒ€ì…: {type(_pipe)}")
                _loaded_key = want
                print(f"[chapterizer] íŒŒì´í”„ë¼ì¸ ë°˜í™˜ ì¤€ë¹„ ì™„ë£Œ")
                return _pipe
            except Exception as e:
                print(f"[chapterizer] 4bit load failed -> fallback FP. reason={e}")
                import traceback
                traceback.print_exc()
                mdl = None

        if mdl is None:
            print(f"[chapterizer] FP ëª¨ë¸ ë¡œë”© ì‹œë„...")
            mdl = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=dtype,
                **common_kw
            )
            print(f"[chapterizer] FP ëª¨ë¸ ë¡œë”© ì„±ê³µ, íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")

        # ì¼ë¶€ ëª¨ë¸ ê²½ê³  ë°©ì§€: pad í† í° ì—†ìœ¼ë©´ eosë¡œ ëŒ€ì²´
        if tok.pad_token_id is None and tok.eos_token_id is not None:
            tok.pad_token_id = tok.eos_token_id

        print(f"[chapterizer] FP íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œì‘...")
        _pipe = pipeline(
            "text-generation",
            model=mdl, tokenizer=tok,
            temperature=temperature, 
            max_new_tokens=max_new_tokens,
            do_sample=True, 
            return_full_text=False,
            pad_token_id=tok.eos_token_id
        )
        print(f"[chapterizer] FP íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")
        print(f"[chapterizer] íŒŒì´í”„ë¼ì¸ ê°ì²´ íƒ€ì…: {type(_pipe)}")
        _loaded_key = want
        print(f"[chapterizer] FP íŒŒì´í”„ë¼ì¸ ë°˜í™˜ ì¤€ë¹„ ì™„ë£Œ")
        return _pipe
    except Exception as global_error:
        print(f"[chapterizer] _get_pipe ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {global_error}")
        import traceback
        traceback.print_exc()
        return None

def _pack_segments_for_prompt(segments: List[Dict[str, Any]], duration: float, max_segments: int) -> str:
    """ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì••ì¶•."""
    segs = segments
    if len(segs) > max_segments:
        step = len(segs) / max_segments
        idxs = [int(i * step) for i in range(max_segments)]
        segs = [segments[i] for i in idxs]
    lines = []
    for s in segs:
        st = round_time(s.get("start", 0.0))
        en = round_time(s.get("end", 0.0))
        tx = (s.get("text") or "").strip()
        lines.append(f"{st:.3f}|{en:.3f}|{tx}")
    return "\n".join(lines)

def _extract_text(outputs: List[Dict[str, Any]]) -> str:
    """pipeline ì¶œë ¥ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ."""
    if not outputs:
        return ""
    o = outputs[0]
    if "generated_text" in o:
        gt = o["generated_text"]
        if isinstance(gt, list) and gt:
            last = gt[-1]
            if isinstance(last, dict):
                content = last.get("content", "")
                if isinstance(content, list):
                    return "".join([c.get("text", "") if isinstance(c, dict) else str(c) for c in content])
                return str(content)
            return str(last)
        if isinstance(gt, str):
            return gt
    if "text" in o:
        return str(o["text"])
    return ""

def _extract_time_boundaries(segments: List[Dict[str, Any]], duration: float, pipe) -> List[Dict[str, float]]:
    """1ë‹¨ê³„: ì†Œì£¼ì œ ê²½ê³„ë§Œ ì¶”ì¶œ (ì‹œê°„ êµ¬ê°„ë§Œ) - MAJOR topics only"""
    import re
    import json
    
    print(f"[1ë‹¨ê³„] ì‹œê°„ êµ¬ê°„ ì¶”ì¶œ ì‹œì‘", flush=True)
    print(f"[1ë‹¨ê³„] ì˜ìƒ ê¸¸ì´: {duration:.1f}ì´ˆ", flush=True)
    print(f"[1ë‹¨ê³„] ìë§‰ ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ", flush=True)
    
    # ì „ì²´ ìë§‰ì„ ì••ì¶•í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    transcript_lines = []
    for s in segments[:200]:  # ìµœëŒ€ 200ê°œë§Œ
        transcript_lines.append(f"{s.get('start', 0):.1f}s: {s.get('text', '').strip()}")
    transcript_text = "\n".join(transcript_lines)
    
    # â˜… ëŒ€í­ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a video topic segmenter. Output ONLY valid JSON.
Your goal is to return 4â€“8 MAJOR topic segments (not subtitle-sized fragments).

Hard constraints:
- Use ONLY timestamps that appear in the transcript; do NOT invent times.
- Segments must be contiguous, non-overlapping, strictly increasing (start < end).
- Each segment length â‰¥ 60.0s. Also enforce a MINIMUM GAP of 45.0s between any two boundaries.
- TOTAL segment count MUST be between 4 and 8 (inclusive). If you detect more, MERGE adjacent segments until the count â‰¤ 8 while keeping â‰¥60.0s.
- Start the first segment at 0.0. Clamp the final segment end to the video duration.
- Prefer boundaries at MAJOR transitions (new section/theme, â€œnow/next/first/in summaryâ€, theoryâ†”demo, new dataset/model, Q&A, recap).

Anti-fragmentation rules:
- DO NOT place boundaries merely because a new subtitle line appears.
- Ignore candidate boundaries that are < 45.0s from the previous boundary.
- If a candidate boundary lacks a clear topical cue, discard it during merging.

Output format (exact key order, numbers with 1 decimal place):
{{"boundaries":[
  {{"start": 0.0, "end": 0.0}}
]}}

Do all reasoning internally. Return ONLY the JSON.
<|eot_id|><|start_header_id|>user<|end_header_id|>

Video duration: {duration:.1f} seconds

Transcript with timestamps (each line starts with a timestamp in seconds or [mm:ss(.ms)]):
{transcript_text}

Task:
1) Propose candidate major-topic boundaries from the transcript (based on topical cues, not subtitle lines).
2) Enforce: min segment length â‰¥60.0s AND min boundary gap â‰¥45.0s.
3) If segments > 8, repeatedly MERGE the weakest-adjacent boundary (fewest/weakest cues) until count â‰¤ 8.
4) Snap boundaries to existing transcript timestamps and clamp the final end to {duration:.1f}.
5) Return ONLY the JSON {{"boundaries":[...]}} with seconds to 1 decimal place.

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    # â˜… max_new_tokens=800, temperature=0.2
    outputs = pipe(prompt, max_new_tokens=800, temperature=0.2)
    text = _extract_text(outputs)
    
    # â˜… ì „ì²´ ì‘ë‹µ ì¶œë ¥
    print(f"[1ë‹¨ê³„] LLM ì‘ë‹µ (ì „ì²´):", flush=True)
    print("=" * 80, flush=True)
    print(text, flush=True)
    print("=" * 80, flush=True)
    
    # â˜… ê°•í™”ëœ JSON íŒŒì‹± ë¡œì§
    try:
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°±/ê°œí–‰ ì œê±°
        json_text = text.strip()
        
        # 2. ë°°ì—´ í˜•ì‹ ì •ê·œí™”
        json_text = re.sub(r',\s*]', ']', json_text)  # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        json_text = re.sub(r',\s*}', '}', json_text)  # ê°ì²´ ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        
        # 3. ë°°ì—´ ì¤‘ê°„ ë¶€ë¶„ ì²˜ë¦¬ ({ ë¡œ ì‹œì‘í•˜ê³  ] ë¡œ ëë‚˜ëŠ” ê²½ìš°)
        if json_text.startswith('{') and json_text.endswith(']'):
            # ë°°ì—´ ì¤‘ê°„ì´ë¯€ë¡œ [ ì¶”ê°€
            json_text = '[' + json_text
        
        # 4. boundaries í‚¤ í™•ì¸ ë° ì¶”ê°€
        if not json_text.startswith('{"boundaries"'):
            if json_text.startswith('['):
                # [ë¡œ ì‹œì‘í•˜ë©´ boundariesë¡œ ê°ì‹¸ê¸°
                json_text = '{"boundaries": ' + json_text + '}'
            elif json_text.startswith('{'):
                # {ë¡œ ì‹œì‘í•˜ëŠ”ë° boundariesê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‹œë„
                pass
            else:
                # ì•„ë¬´ê²ƒë„ ì•„ë‹ˆë©´ boundariesë¡œ ê°ì‹¸ê¸°
                json_text = '{"boundaries": [' + json_text
        
        # 5. ë ë¶€ë¶„ ì •ë¦¬
        if not json_text.endswith('}'):
            if json_text.endswith(']'):
                json_text = json_text + '}'
            elif json_text.endswith(']}'):
                # ì´ë¯¸ ì™„ë£Œëœ í˜•íƒœ
                pass
            else:
                json_text = json_text + ']}'
        
        # 6. Incomplete JSON ì™„ì„± (ìƒˆ ë¡œì§)
        if not json_text.endswith(']}'):
            # ë§ˆì§€ë§‰ ê°ì²´ê°€ ì™„ì„±ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì œê±°
            last_complete = json_text.rfind('},')
            if last_complete != -1:
                print(f"[1ë‹¨ê³„] Incomplete JSON ê°ì§€, ë§ˆì§€ë§‰ ì™„ì „í•œ ê°ì²´ê¹Œì§€ë§Œ ì‚¬ìš©", flush=True)
                json_text = json_text[:last_complete+1] + ']}'
            else:
                # ì²« ê°ì²´ë„ ì™„ì„± ì•ˆëìœ¼ë©´ í¬ê¸°
                print(f"[1ë‹¨ê³„] JSON ì™„ì „ ì‹¤íŒ¨, ë¹ˆ ë°°ì—´ ì‚¬ìš©", flush=True)
                json_text = '{"boundaries": []}'
        
        # 7. íŒŒì‹±
        obj = json.loads(json_text)
        boundaries = obj.get("boundaries", [])
        
        print(f"[1ë‹¨ê³„] ì¶”ì¶œëœ êµ¬ê°„ ìˆ˜: {len(boundaries)}ê°œ", flush=True)
        for i, b in enumerate(boundaries):
            duration_sec = b.get('end', 0) - b.get('start', 0)
            print(f"  {i+1}. {b.get('start', 0):.1f}s ~ {b.get('end', 0):.1f}s (ê¸¸ì´: {duration_sec:.1f}ì´ˆ)", flush=True)
        
        # 8. ë§ˆì§€ë§‰ êµ¬ê°„ ëˆ„ë½ ì²´í¬
        if boundaries:
            last_end = boundaries[-1].get('end', 0)
            if last_end < duration - 30:  # 30ì´ˆ ì´ìƒ ë‚¨ì•˜ìœ¼ë©´
                print(f"[1ë‹¨ê³„] ë§ˆì§€ë§‰ êµ¬ê°„ ì¶”ê°€: {last_end:.1f}s ~ {duration:.1f}s", flush=True)
                boundaries.append({"start": last_end, "end": duration})
        
        return boundaries
    except Exception as e:
        print(f"[1ë‹¨ê³„] JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ë³¸ êµ¬ê°„ ì‚¬ìš©", flush=True)
        # ì‹¤íŒ¨ ì‹œ ê· ë“± ë¶„í• 
        num_chapters = 6
        chunk_duration = duration / num_chapters
        return [{"start": i * chunk_duration, "end": (i + 1) * chunk_duration} for i in range(num_chapters)]

def _validate_and_merge_boundaries(boundaries: List[Dict[str, float]], duration: float, min_duration: float = 60.0) -> List[Dict[str, float]]:
    """êµ¬ê°„ ê²€ì¦ ë° ë³‘í•© - ë„ˆë¬´ ë§ê±°ë‚˜ ì§§ì€ êµ¬ê°„ ì²˜ë¦¬"""
    if not boundaries:
        return boundaries
    
    print(f"\n[ê²€ì¦] êµ¬ê°„ ê²€ì¦ ì‹œì‘ - ì…ë ¥: {len(boundaries)}ê°œ", flush=True)
    
    # 1. ë„ˆë¬´ ë§ì€ êµ¬ê°„ í•„í„°ë§ (> 10ê°œ)
    if len(boundaries) > 10:
        print(f"[ê²½ê³ ] êµ¬ê°„ì´ ë„ˆë¬´ ë§ìŒ ({len(boundaries)}ê°œ) - ìƒìœ„ 8ê°œë§Œ ì‚¬ìš©", flush=True)
        # ê¸´ êµ¬ê°„ ìš°ì„  ì„ íƒ
        boundaries_with_duration = []
        for b in boundaries:
            dur = b.get('end', 0) - b.get('start', 0)
            boundaries_with_duration.append((b, dur))
        
        boundaries_with_duration.sort(key=lambda x: x[1], reverse=True)
        boundaries = [b[0] for b in boundaries_with_duration[:8]]
        boundaries.sort(key=lambda x: x.get('start', 0))
        print(f"[ê²€ì¦] í•„í„°ë§ í›„: {len(boundaries)}ê°œ êµ¬ê°„", flush=True)
    
    # 2. ë„ˆë¬´ ì§§ì€ êµ¬ê°„ ë³‘í•© (< min_duration)
    merged = []
    current = None
    
    for b in boundaries:
        if current is None:
            current = dict(b)  # ë³µì‚¬
        else:
            current_duration = current['end'] - current['start']
            if current_duration < min_duration:
                # í˜„ì¬ êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ìŒ êµ¬ê°„ê³¼ ë³‘í•©
                current['end'] = b.get('end', current['end'])
                print(f"[ê²€ì¦] ë³‘í•©: {current['start']:.1f}s ~ {b.get('end', 0):.1f}s (ë„ˆë¬´ ì§§ìŒ)", flush=True)
            else:
                # í˜„ì¬ êµ¬ê°„ì´ ì¶©ë¶„íˆ ê¸¸ë©´ ì¶”ê°€í•˜ê³  ìƒˆë¡œ ì‹œì‘
                merged.append(current)
                current = dict(b)
    
    # ë§ˆì§€ë§‰ êµ¬ê°„ ì¶”ê°€
    if current:
        merged.append(current)
    
    # 3. ìµœì¢… ê²€ì¦ - duration ì´ˆê³¼ ìˆ˜ì •
    for b in merged:
        if b['end'] > duration:
            print(f"[ê²€ì¦] ì¢…ë£Œ ì‹œê°„ ìˆ˜ì •: {b['end']:.1f}s â†’ {duration:.1f}s", flush=True)
            b['end'] = duration
        if b['start'] >= b['end']:
            b['end'] = min(b['start'] + 60.0, duration)
            print(f"[ê²€ì¦] ì‹œê°„ ë²”ìœ„ ìˆ˜ì •: {b['start']:.1f}s ~ {b['end']:.1f}s", flush=True)
    
    print(f"[ê²€ì¦] ìµœì¢… ê²°ê³¼: {len(merged)}ê°œ êµ¬ê°„", flush=True)
    for i, b in enumerate(merged):
        dur = b['end'] - b['start']
        print(f"  {i+1}. {b['start']:.1f}s ~ {b['end']:.1f}s (ê¸¸ì´: {dur:.1f}ì´ˆ)", flush=True)
    
    return merged

def _generate_chapter_metadata(segments: List[Dict[str, Any]], start: float, end: float, lang: str, pipe) -> Dict[str, str]:
    """2ë‹¨ê³„: í•´ë‹¹ êµ¬ê°„ì˜ ìë§‰ìœ¼ë¡œ ì œëª©/ìš”ì•½ ìƒì„± (ì›ë³¸ ì–¸ì–´)"""
    import re
    import json
    
    # í•´ë‹¹ êµ¬ê°„ì˜ ìë§‰ë§Œ í•„í„°ë§
    chapter_segments = [s for s in segments if s.get('start', 0) >= start and s.get('end', 0) <= end]
    
    if not chapter_segments:
        print(f"[chapterizer] ê²½ê³ : êµ¬ê°„ {start:.1f}s-{end:.1f}sì— ìë§‰ ì—†ìŒ")
        return {"title": "Untitled", "summary": "No content"}
    
    # ìë§‰ í…ìŠ¤íŠ¸ ê²°í•©
    transcript = " ".join([s.get('text', '').strip() for s in chapter_segments])
    
    if not transcript.strip():
        print(f"[chapterizer] ê²½ê³ : êµ¬ê°„ {start:.1f}s-{end:.1f}sì˜ ìë§‰ì´ ë¹„ì–´ìˆìŒ")
        return {"title": "Untitled", "summary": "No content"}
    
    # ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸
    lang_name = _lang_label_from_code(lang)
    
    # â˜… íƒ€ê²Ÿ ì–¸ì–´ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì–¸ì–´ ì„ íƒ
    if lang_name == "Korean":
        # í•œê¸€ í”„ë¡¬í”„íŠ¸
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ êµìœ¡ ì½˜í…ì¸  ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìë§‰ì„ ì½ê³  ë°˜ë“œì‹œ ë‹¤ìŒ JSONì„ ìƒì„±í•˜ì„¸ìš”:

ì¶œë ¥ í˜•ì‹(JSON):
{{"title": "ì œëª©", "summary": "ìš”ì•½ ë‚´ìš©"}}

ì œì•½ ì¡°ê±´:
1. ì œëª©
   - 3~5ê°œì˜ ë‹¨ì–´
   - 7~30ì ì´ë‚´
   - í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼, ë¬¸ì¥ í˜•íƒœ ê¸ˆì§€
   - ë°˜ë“œì‹œ ìˆœìˆ˜ í•œê¸€ë§Œ ì‚¬ìš©
2. ìš”ì•½
   - ë°˜ë“œì‹œ 1ë¬¸ì¥ ë˜ëŠ” 2ë¬¸ì¥ë§Œ ì‘ì„±
   - 1ë¬¸ì¥ì¼ ê²½ìš° ì „ì²´ ê¸¸ì´ 80~120ì
   - 2ë¬¸ì¥ì¼ ê²½ìš° ê° ë¬¸ì¥ì€ 40~60ì, ë‘ ë¬¸ì¥ì˜ í•©ì€ 80~120ì
   - ë§ˆì¹¨í‘œ(.)ëŠ” ìµœëŒ€ ë‘ ë²ˆê¹Œì§€ë§Œ í—ˆìš©
   - í•œ ë²ˆ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ
   - ê°™ì€ ì–´êµ¬ë‚˜ ê°™ì€ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë‘ ë²ˆ ì“°ì§€ ë§ ê²ƒ
   - ë°˜ë“œì‹œ ìˆœìˆ˜ í•œê¸€ë§Œ ì‚¬ìš©
3. ê³µí†µ ê·œì¹™
   - ì¶œë ¥ì—ëŠ” ì˜ì–´, ìˆ«ì, ì•ŒíŒŒë²³, ì™¸ë˜ì–´, ê¸°í˜¸ ì ˆëŒ€ í¬í•¨ ê¸ˆì§€
   - ì™¸ë˜ì–´Â·ì „ë¬¸ ìš©ì–´ë„ í•œê¸€ë¡œ í’€ì–´ì“°ê¸°
   - JSON ê°’ ì•ˆì—ë„ í•œê¸€ë§Œ í¬í•¨ë˜ì–´ì•¼ í•¨
   - JSON ì´ì™¸ì˜ ë‹¤ë¥¸ ê¸€ìë‚˜ ì„¤ëª…ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ

<|eot_id|><|start_header_id|>user<|end_header_id|>

ìë§‰ ë‚´ìš©:
{transcript[:1200]}

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    else:
        # ì˜ì–´ í”„ë¡¬í”„íŠ¸
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are an educational content summarization expert. Read the given transcript and generate the following JSON only:

Output format (JSON):
{"title": "Title", "summary": "Summary content"}

Constraints:
1. Title
   - Must contain 3â€“5 words
   - Length must be between 7â€“30 characters
   - Focus on core keywords, not full sentences
   - Must be written in pure English only
2. Summary
   - Must be exactly 1 or 2 sentences
   - If 1 sentence: total length must be 80â€“120 characters
   - If 2 sentences: each must be 40â€“60 characters, with a combined total of 80â€“120 characters
   - Periods (.) are allowed at most twice
   - Do not repeat the same information once mentioned
   - Do not reuse the same phrase or sentence structure
   - Must be written in pure English only
3. Common rules
   - The output must not contain Korean, numbers, foreign words, or symbols other than English letters and spaces
   - Technical terms should be expressed in plain English words
   - JSON values must contain English text only
   - Do not output anything other than the JSON

<|eot_id|><|start_header_id|>user<|end_header_id|>

Subtitles:
{transcript[:1200]}

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    print(f"[2ë‹¨ê³„] ìë§‰ ê¸¸ì´: {len(transcript)}ì, í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì", flush=True)
    
    # â˜… ìµœì  íŒŒë¼ë¯¸í„°: temperature=0.27 (ë°˜ë³µ ë°©ì§€ + ì˜ì–´ í˜¼ì¬ ìµœì†Œí™”)
    outputs = pipe(
        prompt, 
        max_new_tokens=200, 
        temperature=0.25,
        repetition_penalty=1.1,
        top_p=0.9,
    )
    text = _extract_text(outputs)
    
    print(f"[2ë‹¨ê³„] LLM ì‘ë‹µ (ì²« 200ì): {text[:200]}", flush=True)
    
    # â˜… ê°•í™”ëœ JSON íŒŒì‹± ë¡œì§
    try:
        # 1. ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ìì—´ ì²˜ë¦¬ ("ì£¼ì œ" í˜•ì‹)
        if text.startswith('"') and not text.startswith('{'):
            # "ë¬¸ìì—´" â†’ {"title": "ë¬¸ìì—´", "summary": "ë¬¸ìì—´"}
            cleaned = text.strip('"')
            print(f"[chapterizer] âœ… ë”°ì˜´í‘œ ë¬¸ìì—´ ì²˜ë¦¬ - ì œëª©: {cleaned[:50]}", flush=True)
            return {"title": cleaned, "summary": cleaned}
        
        # 2. JSON ì¶”ì¶œ ê°•í™”
        json_text = text.strip()
        
        # 2.5. "summary" í‚¤ ëˆ„ë½ ì²˜ë¦¬
        if '"title"' in json_text and '"summary"' not in json_text and json_text.startswith('{'):
            # {"title":"...", "í…ìŠ¤íŠ¸..."} í˜•íƒœ
            title_match = re.search(r'"title"\s*:\s*"([^"]+)"', json_text)
            if title_match:
                title = title_match.group(1)
                # title ì´í›„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ summaryë¡œ ì²˜ë¦¬
                after_title = json_text[title_match.end():]
                # , " ë‹¤ìŒë¶€í„° ì‹œì‘ (summary í‚¤ ì—†ì´ ë°”ë¡œ ë‚´ìš©)
                content_match = re.search(r',\s*"([^"]+)', after_title)
                if content_match:
                    summary = content_match.group(1)
                    print(f"[chapterizer] ğŸ”§ summary í‚¤ ëˆ„ë½ ìˆ˜ì • - ì œëª©: {title[:30]}", flush=True)
                    return {"title": title, "summary": summary}
        
        json_text = re.sub(r',\s*}', '}', json_text)  # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        
        # 3. Unterminated string ì²˜ë¦¬ ("}ê°€ ì—†ìœ¼ë©´ ì¶”ê°€)
        if json_text.startswith('{') and not json_text.endswith('}'):
            # "summary": "í…ìŠ¤íŠ¸... í˜•íƒœì—ì„œ " ì¶”ê°€
            if '"summary"' in json_text and json_text.count('"') % 2 == 1:
                json_text = json_text + '"}'
            else:
                json_text = json_text + '}'
        
        # 4. { ì•ì˜ ëª¨ë“  ë¬¸ì ì œê±°
        json_start = json_text.find('{')
        if json_start > 0:
            json_text = json_text[json_start:]
        
        # 5. } ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±°
        json_end = json_text.rfind('}')
        if json_end != -1:
            json_text = json_text[:json_end+1]
        
        # 6. JSON íŒŒì‹±
        if json_text:
            obj = json.loads(json_text)
            title = obj.get("title", "").strip()
            summary = obj.get("summary", "").strip()
            
            if title and summary:
                print(f"[chapterizer] âœ… JSON íŒŒì‹± ì„±ê³µ - ì œëª©: {title[:50]}", flush=True)
                return {"title": title, "summary": summary}
            else:
                raise ValueError("ì œëª© ë˜ëŠ” ìš”ì•½ ë¹„ì–´ìˆìŒ")
        else:
            raise ValueError("JSON ì¶”ì¶œ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"[chapterizer] âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}, Regex fallback ì‹œë„...", flush=True)
        
        # â˜… Regex fallback
        title_match = re.search(r'"title"\s*:\s*"([^"]+)"', text)
        summary_match = re.search(r'"summary"\s*:\s*"([^"]+)"', text)
        
        if title_match and summary_match:
            title = title_match.group(1).strip()
            summary = summary_match.group(1).strip()
            print(f"[chapterizer] âœ… Regex íŒŒì‹± ì„±ê³µ - ì œëª©: {title[:50]}", flush=True)
            return {"title": title, "summary": summary}
        
        # â˜… ìµœì¢… fallback - ì˜ë¯¸ìˆëŠ” ìš”ì•½
        print(f"[chapterizer] âŒ íŒŒì‹± ì™„ì „ ì‹¤íŒ¨, ìµœì¢… fallback ì‚¬ìš©", flush=True)
        sentences = transcript.split('. ')
        first_sentence = sentences[0][:200] if sentences else transcript[:200]
        
        return {
            "title": f"Part {int(start/60)+1}",
            "summary": first_sentence
        }

def make_chapters_hf(
    *,
    segments: List[Dict[str, Any]],
    duration: float,
    lang: Optional[str],                 # Whisper ê°ì§€ ì–¸ì–´ì½”ë“œ
    model_id: str,
    load_in_4bit: bool,
    temperature: float,
    max_new_tokens: int,
    max_segments_for_prompt: int,
    hf_token: Optional[str] = None,
    # ë©”ëª¨ë¦¬/ì˜¤í”„ë¡œë”© ì¸ì
    max_gpu_mem: str = "18GiB",
    max_cpu_mem: str = "32GiB",
    offload_dir: str = "./offload",
    low_cpu_mem: bool = True,
    torch_dtype_name: str = "auto",
) -> List[Dict[str, Any]]:
    """â˜… NEW: 2ë‹¨ê³„ ì±•í„° ìƒì„± ë°©ì‹ - êµ¬ê°„ ì¶”ì¶œ í›„ ì œëª©/ìš”ì•½ ìƒì„±"""
    print(f"[chapterizer] 2ë‹¨ê³„ ë¶„ì„ ì‹œì‘ - ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ, ì–¸ì–´: {lang}", flush=True)
    print(f"[chapterizer] ëª¨ë¸ ID: {model_id}", flush=True)
    
    try:
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print(f"[chapterizer] ëª¨ë¸ ë¡œë”© ì¤‘...", flush=True)
        pipe = _get_pipe(
            model_id, load_in_4bit, 0.3, max_new_tokens, hf_token,
            max_gpu_mem=max_gpu_mem,
            max_cpu_mem=max_cpu_mem,
            offload_dir=offload_dir,
            low_cpu_mem=low_cpu_mem,
            torch_dtype_name=torch_dtype_name
        )
        
        if pipe is None:
            print(f"[chapterizer] ì˜¤ë¥˜: íŒŒì´í”„ ë¡œë“œ ì‹¤íŒ¨!", flush=True)
            return []
        
        print(f"[chapterizer] ëª¨ë¸ ë¡œë”© ì™„ë£Œ", flush=True)
        
        # â˜… 1ë‹¨ê³„: ì‹œê°„ êµ¬ê°„ ì¶”ì¶œ
        boundaries = _extract_time_boundaries(segments, duration, pipe)
        
        if not boundaries:
            print(f"[chapterizer] ê²½ê³ : êµ¬ê°„ ì¶”ì¶œ ì‹¤íŒ¨, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜", flush=True)
            return []
        
        # â˜… êµ¬ê°„ ê²€ì¦ ë° ë³‘í•©
        boundaries = _validate_and_merge_boundaries(boundaries, duration, min_duration=60.0)
        
        # â˜… 2ë‹¨ê³„: ê° êµ¬ê°„ì˜ ì œëª©/ìš”ì•½ ìƒì„±
        print(f"\n[chapterizer] 2ë‹¨ê³„: ê° êµ¬ê°„ì˜ ì œëª©/ìš”ì•½ ìƒì„± ì‹œì‘ ({len(boundaries)}ê°œ êµ¬ê°„)", flush=True)
        print("=" * 80, flush=True)
        chapters = []
        success_count = 0
        fallback_count = 0
        
        for i, boundary in enumerate(boundaries):
            start = float(boundary.get('start', 0))
            end = float(boundary.get('end', duration))
            
            # ì‹œê°„ ë²”ìœ„ ê²€ì¦
            start = max(0.0, min(start, duration))
            end = max(start + 10.0, min(end, duration))  # ìµœì†Œ 10ì´ˆ ë³´ì¥
            
            print(f"\n  [{i+1}/{len(boundaries)}] êµ¬ê°„ ë¶„ì„: {start:.1f}s ~ {end:.1f}s (ê¸¸ì´: {end-start:.1f}ì´ˆ)", flush=True)
            
            # í•´ë‹¹ êµ¬ê°„ì˜ ì œëª©/ìš”ì•½ ìƒì„±
            metadata = _generate_chapter_metadata(segments, start, end, lang, pipe)
            
            title = metadata.get("title", "Untitled")
            summary = metadata.get("summary", "")
            
            chapters.append({
                "start": start,
                "end": end,
                "title": title,
                "summary": summary
            })
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
            if title.startswith("Part ") or title.startswith("Chapter "):
                fallback_count += 1
                print(f"     âš ï¸ Fallback ì‚¬ìš©ë¨", flush=True)
            else:
                success_count += 1
                print(f"     âœ… ì„±ê³µ", flush=True)
            
            print(f"     ì œëª©: {title}", flush=True)
            print(f"     ìš”ì•½: {summary[:100]}{'...' if len(summary) > 100 else ''}", flush=True)
        
        print(f"\n{'=' * 80}", flush=True)
        print(f"[ì±•í„° 2ë‹¨ê³„ ë¶„ì„ ì™„ë£Œ]", flush=True)
        print(f"[ìƒì„±ëœ ì±•í„° ìˆ˜] {len(chapters)}ê°œ", flush=True)
        print(f"[ì„±ê³µ] {success_count}ê°œ / [Fallback] {fallback_count}ê°œ", flush=True)
        
        if chapters:
            print(f"\n[ì±•í„° ëª©ë¡]", flush=True)
            for i, chapter in enumerate(chapters):
                print(f"  {i+1}. [{chapter['start']:.2f}s - {chapter['end']:.2f}s] {chapter['title']}", flush=True)
        
        print(flush=True)
        return chapters
        
    except Exception as e:
        print(f"[chapterizer] 2ë‹¨ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return []
