# LLM
LLM_PROVIDER=hf_local
HF_MODEL_ID=meta-llama/Llama-3.2-3B-Instruct

# Linux + CUDA?먯꽌 4bit ?쒕룄
HF_LOAD_IN_4BIT=true

# GPU/CPU 硫붾え由??꾨줈?뚯씪
HF_MAX_NEW_TOKENS=512
HF_MAX_GPU_MEMORY=18GiB
HF_MAX_CPU_MEMORY=16GiB

# ?쇰컲 ?ㅼ젙
HF_TEMPERATURE=0.2
HF_OFFLOAD_DIR=/offload
HF_LOW_CPU_MEM=true
HF_TORCH_DTYPE=auto

# Whisper
WHISPER_MODEL=small
WHISPER_FP16=true

# Hugging Face ?좏겙 (?ш린留? 源껎뿀釉?而ㅻ컠 湲덉?)
HF_TOKEN=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

# ?몄쓽??理쒖쟻??PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64

# 怨듭슜
TZ=Asia/Seoul
HF_HOME=/models/hf
PIP_CACHE_DIR=/pipcache

TRANSFORMERS_NO_MXFP4=1
HF_USE_QUANTO=0
